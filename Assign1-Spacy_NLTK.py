import nltk
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer

# Download required NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# üî§ Sample Text Input
sample_text = """
Hello this is a sample text !. Ok !
"""

# ‚úÖ Step 1: Tokenization
tokens = word_tokenize(sample_text)
print("1Ô∏è‚É£ Tokens:")
print(tokens)

# ‚úÖ Step 2: Remove Punctuation
tokens_no_punct = [word for word in tokens if word not in string.punctuation]
print("\n2Ô∏è‚É£ After Punctuation Removal:")
print(tokens_no_punct)

# ‚úÖ Step 3: Remove Stopwords
stop_words = set(stopwords.words('english'))
tokens_no_stopwords = [word for word in tokens_no_punct if word.lower() not in stop_words]
print("\n3Ô∏è‚É£ After Stop Word Removal:")
print(tokens_no_stopwords)

# ‚úÖ Step 4: Stemming
# Using PorterStemmer
porter = PorterStemmer()
porter_stemmed = [porter.stem(word) for word in tokens_no_stopwords]
print("\n4Ô∏è‚É£ Porter Stemmer:")
print(porter_stemmed)

# Using LancasterStemmer (more aggressive)
lancaster = LancasterStemmer()
lancaster_stemmed = [lancaster.stem(word) for word in tokens_no_stopwords]
print("\n4Ô∏è‚É£ Lancaster Stemmer:")
print(lancaster_stemmed)

# Using SnowballStemmer
snowball = SnowballStemmer("english")
snowball_stemmed = [snowball.stem(word) for word in tokens_no_stopwords]
print("\n4Ô∏è‚É£ Snowball Stemmer:")
print(snowball_stemmed)

# ‚úÖ Step 5: Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in tokens_no_stopwords]
print("\n5Ô∏è‚É£ Lemmatized Words:")
print(lemmatized_words)
